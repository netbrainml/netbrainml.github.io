{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Workshop 2: Approximation Methods (Part 1).\n\n*    Notebook for Workshop 2: Approximation Methods (Part 1).\n*    Other parts of the workshops are available and can be found [here](https://netbrainml.github.io/workshops/).\n\n### Table of Content\n\n   1. [Value Approximation Methods](#vam)\n   \n    *[Gradient MC Method](#grad_mc)\n\n    *[Semi-gradient TD(0)](#semigrad_td)\n    \n    *[Cartpole Environment](#env)\n    \n    *[DQN](#dqn)\n       \n   2. [Policy Approximation Methods](#pam)\n       \n    *[REINFORCE](#reinforce)\n     \n    *[Cross Entropy](#cemes)\n             \n   3. [Actor Critic Approximation Methods](#acam)\n       \n    *[Actor Critic](#ac)\n     \n    *[Advantage Actor Critic](#a2c)\n     \n   4. [Model-based Approximation Methods](#mam)\n       \n    *[DYNA-Q](#dynaq)\n     "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pip\ndef import_or_install(package):\n    try: __import__(package)\n    except ImportError: pip.main(['install', package]) \nmodules = ['numpy','collections','functools', 'gym']\nfor m in modules:\n    import_or_install(m)\nfrom IPython.display import clear_output\nclear_output(wait=False) ","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Value Approximation Methods <a name=\"vam\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport random\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_normal(m)\n        \nclass Value(nn.Module):\n    def __init__(self,ni,nh,action_space=[0,1],device = 'cpu'):\n        super(Value,self).__init__()\n        self.linear = nn.Sequential(nn.Linear(ni,nh),\n                                    nn.Tanh(),\n                                    nn.Linear(nh,1))\n        self.action_space = action_space\n        self.device = device\n        self.to(self.device)\n        init_weights(self)\n        \n    def forward(self,obs,action,batch=False):\n        if batch:\n            convert = lambda x: np.array(x) if type(x)!=np.array else x\n            bo = convert(obs)\n            ba = convert(action)\n            state_action = torch.tensor(np.hstack([bo, ba.reshape(-1,1)])).float().to(self.device)\n        else:\n            state_action = self.create_pair(obs,action)\n        return self.linear(state_action)\n    \n    def act(self,obs,epilson=0.2,batch=False):\n        if batch:\n            if random.random()<epilson: return np.random.choice(self.action_space, len(obs))\n            \n            return np.argmax([self(obs,np.ones(len(obs))*action,True).detach().numpy() for action in self.action_space],0)\n        if random.random()<epilson: return np.random.choice(self.action_space)\n        return np.argmax([self(obs,action).item() for action in self.action_space])\n\n    def create_pair(self,obs,action): return torch.tensor(np.append(obs,action)).float().to(self.device)\n\nenv = gym.make(\"Blackjack-v0\")\ndef run(env,policy,k=5000):\n    win = 0\n    for _ in range(k):\n        done=False\n        obs = env.reset()\n        while not done:\n            action = policy(obs)\n            next_obs, reward, done, _ = env.step(action)\n            if reward==1:\n                win+= 1\n    print(f\"Win ratio = {win/k}\")\n\nvalue = Value(3+1, 128)\npolicy = lambda obs: value.act(obs, epilson=0.0)\nrun(env, policy)","execution_count":2,"outputs":[{"output_type":"stream","text":"Win ratio = 0.0026\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Gradient MC Method <a name='grad_mc'>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim import Adam\nfrom tqdm import tqdm\n\ndef grad_mc(env, k=1, gamma=0.9, lr=1e-3, device = 'cpu'):\n    ni = 3 + 1\n    nh = 64\n    value = Value(ni, nh, device = device)\n    optimizer = Adam(value.parameters(), lr = lr)\n    pb = tqdm(range(k))\n    for i in pb:\n        value.train()\n        policy = lambda obs: value.act(obs, epilson = 1/(i+1))\n        episode = create_episode(env,policy)\n        G = 0\n        loss = 0\n        for obs, action, reward in reversed(episode):\n            G = gamma*G + reward\n            yhat = value(obs,action)\n            loss += 0.5*(G - yhat)**2\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()  \n    value.eval()\n    policy = lambda obs: value.act(obs,epilson=0.0)\n    return policy\n\ndef create_episode(env, policy):\n    obs = env.reset()\n    action = policy(obs)\n    next_obs, reward, done, _ = env.step(action)\n    episode = [(next_obs,action,reward)]\n    while not done:\n        obs = next_obs\n        action = policy(obs)\n        next_obs, reward, done, _ = env.step(action)\n        episode.append((next_obs,action,reward))\n    return episode\n\nenv = gym.make(\"Blackjack-v0\")\npolicy = grad_mc(env,10000)\nrun(env, policy)","execution_count":3,"outputs":[{"output_type":"stream","text":"100%|██████████| 10000/10000 [00:18<00:00, 528.64it/s]\n","name":"stderr"},{"output_type":"stream","text":"Win ratio = 0.299\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Semi-Gradient SARSA <a name='semigrad_td'>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\ndef semigrad_sarsa(env, k=1, gamma=0.9, lr=1e-3, device = 'cpu'):\n    ni = 3 + 1\n    nh = 64\n    value = Value(ni, nh, device = device)\n    optimizer = Adam(value.parameters(), lr = lr)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    for i in pb:\n        value.train()\n        action = value.act(obs, epilson=1/(i+1))\n        next_obs, reward, done, _ = env.step(action)\n        next_action = value.act(next_obs, epilson=0.0)\n        target = reward + gamma*value(next_obs,next_action).detach()\n        yhat = value(obs,action)\n        loss = F.mse_loss(target,yhat)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()  \n        obs = next_obs\n        if done:\n            obs = env.reset()\n    value.eval()\n    policy = lambda obs: value.act(obs,epilson=0)\n    return policy\n\nenv = gym.make(\"Blackjack-v0\")\npolicy = semigrad_sarsa(env,10000)\nrun(env, policy)","execution_count":4,"outputs":[{"output_type":"stream","text":"100%|██████████| 10000/10000 [00:22<00:00, 440.76it/s]\n","name":"stderr"},{"output_type":"stream","text":"Win ratio = 0.2082\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Cartpole Environment <a name='env'>\n![](https://i.pinimg.com/originals/6d/dd/7b/6ddd7bd33570419be86147eeb4c9d7ad.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport random\n\nenv = gym.make(\"CartPole-v0\")\n\"\"\"\nCartpole Environment\n\n    Observation:\n        Type: Box(4)\n        Num     Observation               Min                     Max\n        0       Cart Position             -4.8                    4.8\n        1       Cart Velocity             -Inf                    Inf\n        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n        3       Pole Angular Velocity     -Inf                    Inf\n    Actions:\n        Type: Discrete(2)\n        Num   Action\n        0     Push cart to the left\n        1     Push cart to the right\n    Reward:\n        Reward is 1 for every step taken, including the termination step\n    Starting State:\n        All observations are assigned a uniform random value in [-0.05..0.05]\n    Episode Termination:\n        Pole Angle is more than 12 degrees.\n        Cart Position is more than 2.4 (center of the cart reaches the edge of\n        the display).\n        Episode length is greater than 200.\n        Solved Requirements:\n        Considered solved when the average return is greater than or equal to\n        195.0 over 100 consecutive trials.\n\"\"\"\n\ndef run(env,policy,k=100):\n    crewards = 0\n    for _ in range(k):\n        done=False\n        obs = env.reset()\n        while not done:\n            action = policy(obs)\n            next_obs, reward, done, _ = env.step(action)\n            crewards += reward\n            obs = next_obs\n    print(f\"Avg. Reward per Attempt = {crewards/k}\")\n\nvalue = Value(4+1, 128)\npolicy = lambda obs: value.act(obs, epilson=0.0)\nrun(env, policy)","execution_count":5,"outputs":[{"output_type":"stream","text":"Avg. Reward per Attempt = 9.45\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Experience Replay"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import deque\nimport random\n\nclass ExperienceReplay(object):\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    def sample(self,nsamples=64):\n        indices = np.random.choice(np.arange(0,len(self.buffer)),nsamples)\n        samples = np.array(self.buffer)[indices]\n        return samples\n    def add(self,state, action, reward, next_state, done):\n        self.buffer.append([state, action, reward, next_state, done])\n    \nexperience_replay = ExperienceReplay(2)\nenv = gym.make(\"CartPole-v0\")\n\ndone= False\nobs = env.reset()\nfor i in range(4):\n    action = env.action_space.sample()\n    next_obs, reward, done, _ = env.step(action)\n    experience_replay.add(obs, action, reward, next_obs, done)\n    obs = next_obs\n    print(experience_replay.buffer[0])","execution_count":6,"outputs":[{"output_type":"stream","text":"[array([-0.03018639, -0.00358102,  0.04893149,  0.00536381]), 1, 1.0, array([-0.03025801,  0.19080628,  0.04903877, -0.27148802]), False]\n[array([-0.03018639, -0.00358102,  0.04893149,  0.00536381]), 1, 1.0, array([-0.03025801,  0.19080628,  0.04903877, -0.27148802]), False]\n[array([-0.03025801,  0.19080628,  0.04903877, -0.27148802]), 1, 1.0, array([-0.02644188,  0.38519541,  0.04360901, -0.54830957]), False]\n[array([-0.02644188,  0.38519541,  0.04360901, -0.54830957]), 1, 1.0, array([-0.01873798,  0.57967848,  0.03264282, -0.82693984]), False]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Deep Q-Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport random\nimport torch.nn.functional as F\n\nclass DQN(nn.Module):\n    def __init__(self,ni,nh,action_space = [0,1], capacity=1000000,device='cpu'):\n        super(DQN,self).__init__()\n        self.value = Value(ni,nh, action_space=action_space,device = device)\n        self.experience_replay = ExperienceReplay(capacity)\n        self.to(device)\n        \n    def forward(self, obs, action, batch=False):\n        return self.value(obs,action,batch=batch)\n    \n    def act(self,obs,epilson=0.2, batch=False):\n        return self.value.act(obs,epilson=epilson,batch=batch)\n    \ndef dqn_train(env, N, sample_size = 1, k = 1, gamma=0.9, lr = 1e-3, device='cpu'):\n    ni = env.observation_space.shape[0] + 1\n    nh = 128\n    dqn = DQN(ni,nh,capacity=N,device=device)\n    optimizer= Adam(dqn.parameters(), lr=lr)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    creward = 0\n    for i in pb:\n        dqn.train()\n        action = dqn.act(obs,epilson=1/(i+1)+0.1)\n        next_obs, reward, done, _ = env.step(action)\n        dqn.experience_replay.add(obs, action, reward, next_obs, done)\n        obs = next_obs\n        creward += reward\n        if done:\n            pb.set_description(f\"{creward}\")\n            obs = env.reset()\n            creward = 0\n        if i>sample_size:\n            e = dqn.experience_replay.sample(sample_size)\n            obs_, action_, reward_, next_obs_, done_ = [np.stack(e[:,i]) for i in range(5)]\n            values = dqn(obs_, action_, batch=True)\n            next_actions = dqn.act(next_obs_, batch=True)\n            targets = torch.tensor(reward_).unsqueeze(1) + gamma * dqn(next_obs_, next_actions, batch=True) * torch.tensor(1-done_).unsqueeze(1)\n            loss = F.mse_loss(targets,values)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    dqn.eval()\n    policy = lambda obs: dqn.act(obs, epilson=0.0)\n\n    return policy\n\nenv = gym.make(\"CartPole-v0\")\npolicy = dqn_train(env,k=5000,sample_size=256, N=100000)\nrun(env,policy)","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb401df8a3ec423aabd4f082070aee94"}},"metadata":{}},{"output_type":"stream","text":"\nAvg. Reward per Attempt = 92.76\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Policy Approximation Methods <a name=\"pam\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.kaiming_normal(m)\n        \nclass Policy(nn.Module):\n    def __init__(self,ni,nh,no,device = 'cpu'):\n        super(Policy,self).__init__()\n        self.linear = nn.Sequential(nn.Linear(ni,nh),\n                                    nn.ReLU(),\n                                    nn.Linear(nh,no))\n        self.device = device\n        self.to(self.device)\n        init_weights(self)\n        self.no = no\n        \n    def forward(self,obs, batch=False):\n        convert = lambda x: torch.tensor(x).float() if type(x)!=torch.tensor else x\n        obs = convert(obs)\n        return F.softmax(self.linear(obs), 1 if batch else 0)\n    \n    def act(self,obs, batch=False):\n        log_prob = self(obs, batch = batch).detach().numpy()\n        return np.array([np.random.choice(np.arange(self.no),p=p) for p in log_probs]) if batch else np.random.choice(np.arange(self.no),p=log_prob)\n\npolicy = Policy(4,64,2)\nenv = gym.make(\"CartPole-v0\")\nprint(policy(env.reset()))\ndef run(env,policy,k=100):\n    crewards = 0\n    for _ in range(k):\n        done=False\n        obs = env.reset()\n        while not done:\n            action = policy.act(obs)\n            next_obs, reward, done, _ = env.step(action)\n            crewards += reward\n            obs = next_obs\n    print(f\"Avg. Reward per Attempt = {crewards/k}\")\nrun(env, policy)","execution_count":8,"outputs":[{"output_type":"stream","text":"tensor([0.5659, 0.4341], grad_fn=<SoftmaxBackward>)\nAvg. Reward per Attempt = 22.02\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## REINFORCE <a name=\"reinforce\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reinforce(env, k = 1, gamma=0.99, lr = 1e-3, device='cpu', verbose=False):\n    nh = 128\n    policy = Policy(4,nh,2)\n    optimizer= Adam(policy.parameters(), lr=lr)\n    pb = tqdm(range(k)) if verbose else range(k)\n    obs = env.reset()\n    for i in pb:\n        policy.train()\n        episode = create_episode(env,policy)\n        creward = 0\n        G = []\n        R = 0\n        for obs, log_prob, action, reward in reversed(episode):\n            R = gamma* R + reward\n            G.insert(0, R)\n            creward +=reward        \n        loss=0\n        for R,(obs, log_prob, action, reward) in zip(G,episode):\n            loss -= R*torch.log(log_prob)\n    \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step() \n    policy.eval()\n    return policy\n\ndef create_episode(env, policy):\n    obs = env.reset()\n    log_prob = policy(obs)\n    action = policy.act(obs)\n    next_obs, reward, done, _ = env.step(action)\n    episode = [(next_obs,log_prob[action],action,reward)]\n    while not done:\n        obs = next_obs\n        log_prob = policy(obs)\n        action = policy.act(obs)\n        next_obs, reward, done, _ = env.step(action)\n        episode.append((next_obs,log_prob[action],action,reward))\n    return episode\n\npolicy = reinforce(env,k=500,verbose=True)\nrun(env,policy)","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff338e71fdd44b297199ca01d50d3bc"}},"metadata":{}},{"output_type":"stream","text":"\nAvg. Reward per Attempt = 190.55\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Cross Entropy Method <a name=\"cemes\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom torch.distributions.normal import Normal\n\ndef create_policy(ni,nh,no,ref):\n    policy = Policy(ni,nh,no)\n    for layer in policy.linear:\n        if hasattr(layer,\"weight\"):\n            layer.weight.data = Normal(ref[str(layer)]['mu'],\n                                       ref[str(layer)]['std']).rsample()\n    return policy\n\nref = defaultdict(dict)\nfor layer in policy.linear:\n    if hasattr(layer,\"weight\"):\n        ref[str(layer)] = {'mu':torch.rand(layer.weight.data.shape),\n                           \"std\":torch.rand(layer.weight.data.shape)}","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.distributions.normal import Normal\n\ndef cross_entropy(env, k = 1, pop_size=100, nelites = 10, device='cpu', verbose=False):\n    nh = 32\n    pb = tqdm(range(k)) if verbose else range(k)\n    \n    pop = [Policy(4,nh,2) for _ in range(pop_size)]    \n    ref = defaultdict(dict)\n    for i in pb:\n        scores = run_episode(env,pop)\n        if verbose: pb.set_description(f\"{np.mean(scores)}\")\n        elites_args = np.argsort(scores)\n        elites = np.array(pop)[elites_args][-nelites:]\n        \n        #Get all the weights\n        tmp = defaultdict(list)\n        for p in elites:\n            for layer in p.linear:\n                if hasattr(layer,\"weight\"):\n                    tmp[str(layer)].append(layer.weight.data)\n        #Calculate mean and std\n        for k,v in tmp.items():\n            weights = torch.stack(tmp[k])\n            ref[k]['mu'] = torch.mean(weights,0)\n            ref[k]['std'] = torch.std(weights,0)\n        \n        pop = [create_policy(4,nh,2,ref) for _ in range(pop_size)]\n    return pop,scores\n        \ndef run_episode(env, policies):\n    scores = np.zeros(len(policies))\n    for i,policy in enumerate(policies):\n        done = False\n        obs = env.reset()\n        while not done:\n            action = policy.act(obs)\n            obs, reward, done, _ = env.step(action)\n            scores[i]+=reward\n    return scores\n\npop,scores = cross_entropy(env,k=100,verbose=True)\npolicy = pop[np.argmax(scores)]\nrun(env,policy)","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7e74fbfbd049a58e3ddaba667932e0"}},"metadata":{}},{"output_type":"stream","text":"\nAvg. Reward per Attempt = 45.46\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Actor Critic Approximation Methods <a name=\"acam\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ActorCritic(nn.Module):\n    def __init__(self,ni,nh,no,device='cpu'):\n        super(ActorCritic,self).__init__()\n        self.actor = nn.Sequential(nn.Linear(ni,nh),\n                                    nn.LeakyReLU(),\n                                    nn.Linear(nh,no),\n                                    nn.Softmax(0))\n        self.critic = nn.Sequential(nn.Linear(ni,nh),\n                                    nn.LeakyReLU(),\n                                    nn.Linear(nh,1))\n        self.to(device)\n        init_weights(self)\n        self.no=no\n        \n    def forward(self,obs):\n        obs= torch.tensor(obs).float()\n        prob = self.actor(obs)\n        action = np.random.choice(np.arange(self.no),p=prob.detach().numpy())\n        value = self.critic(obs)\n        return action, torch.log(prob[action]), value\n    \n    def act(self,obs):\n        return self(obs)[0]\n    \n    def fit(self,env,gamma = 0.99,k = 1,lr = 1e-4):\n        optimizer= Adam(self.parameters(), lr=lr)\n        pb = tqdm(range(k))\n        for i in pb:\n            self.train()\n            obs = env.reset()\n            done = False\n            log_probs = []\n            rewards = []\n            values = []\n            while not done:\n                action, log_prob, value = self(obs)\n                obs, reward, done, _ = env.step(action)\n                log_probs.append(log_prob)\n                values.append(value)\n                rewards.append(reward)\n                \n            returns = []\n            R = 0\n            for r in reversed(rewards):\n                R = r + gamma * R\n                returns.insert(0, R)\n            returns = np.array(returns)\n            loss = 0\n            for log_prob, value, R in zip(log_probs,values, returns):\n                loss += value.detach() * -log_prob + F.smooth_l1_loss(torch.tensor([R]),value)\n                \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\nenv = gym.make(\"Blackjack-v0\")\ndef run(env,policy,k=5000):\n    wins = 0\n    for _ in range(k):\n        done=False\n        obs = env.reset()\n        while not done:\n            action = policy.act(obs)\n            next_obs, reward, done, _ = env.step(action)\n            obs = next_obs\n            if reward>=1:\n                wins += 1\n    print(f\"Win Percentage = {wins/k}\")\nni = 3\nno = env.action_space.n\nnh = 64\n\nmodel = ActorCritic(ni,nh,no)\nmodel.fit(env, k = 5000)\nrun(env, model)","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501d38011d884701862bf833636de60c"}},"metadata":{}},{"output_type":"stream","text":"\nWin Percentage = 0.3852\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## A2C <a name=\"a2c\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"class A2C(ActorCritic):\n    def __init__(self,ni,nh,no,device='cpu'):\n        super(A2C,self).__init__(ni,nh,no,device)\n        \n    def fit(self,env,gamma = 0.99,k = 1,lr = 1e-3):\n        optimizer= Adam(self.parameters(), lr=lr)\n        pb = tqdm(range(k))\n        for i in pb:\n            self.train()\n            obs = env.reset()\n            done = False\n            log_probs = []\n            rewards = []\n            values = []\n            while not done:\n                action, log_prob, value = self(obs)\n                obs, reward, done, _ = env.step(action)\n                log_probs.append(log_prob)\n                values.append(value)\n                rewards.append(reward)\n            \n            returns = []\n            R = 0\n            for r in reversed(rewards):\n                R = r + gamma * R\n                returns.insert(0, R)\n            returns = np.array(returns)\n            \n            loss = 0\n            for log_prob, value, R in zip(log_probs,values, returns):\n                advantage = R - value.item()\n                loss += advantage * -log_prob + F.smooth_l1_loss(torch.tensor([R]),value)\n                \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            pb.set_description(str(sum(rewards)))\n\nenv = gym.make(\"CartPole-v0\")\ndef run(env,policy,k=100):\n    crewards = 0\n    for _ in range(k):\n        done=False\n        obs = env.reset()\n        while not done:\n            action = policy.act(obs)\n            next_obs, reward, done, _ = env.step(action)\n            obs = next_obs\n            crewards += reward\n    print(f\"Average crewards= {crewards/k}\")\nni = 4\nno = env.action_space.n\nnh = 256\n\nmodel = A2C(ni,nh,no)\nmodel.fit(env, k = 300)\nrun(env, model)","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502e4c61a2cf4469a950d8c01fa664e2"}},"metadata":{}},{"output_type":"stream","text":"\nAverage crewards= 180.56\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Model-based Approximation Method <a name=\"mam\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, ni, nh,device='cpu'):\n        super(Model,self).__init__()\n        self.next_obs = nn.Sequential(nn.Linear(ni,nh),\n                                      nn.LeakyReLU(),\n                                      nn.Linear(nh,ni-1))\n        self.reward = nn.Sequential(nn.Linear(ni,nh),\n                                      nn.Tanh(),\n                                      nn.Linear(nh,1))\n        self.device = device\n    def forward(self,obs,action,batch=False):\n        if batch:\n            convert = lambda x: np.array(x) if type(x)!=np.array else x\n            bo = convert(obs)\n            ba = convert(action)\n            state_action = torch.tensor(np.hstack([bo, ba.reshape(-1,1)])).float().to(self.device)\n        else:\n            state_action = self.create_pair(obs,action)\n        return self.next_obs(state_action), self.reward(state_action)\n    def create_pair(self,obs,action): return torch.tensor(np.append(obs,action)).float().to(self.device)\n\nenv = gym.make(\"CartPole-v0\")\n\nmodel = Model(4+1,64)\nobs = env.reset()\nmodel(obs,1)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"(tensor([-0.1192,  0.1658,  0.0411,  0.2891], grad_fn=<AddBackward0>),\n tensor([0.0614], grad_fn=<AddBackward0>))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## DYNA-Q <a name=\"dynaq\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dyna_q(env, n_planning=10, k=1, gamma=0.9, lr=1e-3, device = 'cpu'):\n    samples = []\n    ni = 3 + 1\n    nh = 64\n    value = Value(ni, nh, device = device)\n    model = Model(ni, nh, device = device)\n    \n    value_optimizer = Adam(value.parameters(), lr = lr)\n    model_optimizer = Adam(model.parameters(), lr = lr)\n\n    obs = env.reset()\n    pb = tqdm(range(k))\n    for i in pb:\n        value.train()\n        action = value.act(obs, epilson=1/(i+1))\n        next_obs, reward, done, _ = env.step(action)\n        next_action = value.act(next_obs, epilson=0.0)\n        target = reward + gamma*value(next_obs,next_action).detach()\n        yhat = value(obs,action)\n        loss = F.mse_loss(target,yhat)\n        value_optimizer.zero_grad()\n        loss.backward()\n        value_optimizer.step()  \n        \n        _next_obs, _reward = model(obs,action)\n        loss = F.mse_loss(torch.tensor(next_obs),_next_obs) + F.mse_loss(torch.tensor([reward]),_reward)\n        model_optimizer.zero_grad()\n        loss.backward()\n        model_optimizer.step()\n        \n        samples.append((obs,action))\n        if len(samples)>n_planning:\n            for j in range(n_planning):\n                _obs, _action = random.choice(samples)\n                _next_obs, _reward = model(_obs, _action)\n                next_action = value.act(next_obs, epilson=0.0)\n                target = reward + gamma*value(next_obs,next_action).detach()\n                yhat = value(obs,action)\n                loss = F.mse_loss(target,yhat)\n                value_optimizer.zero_grad()\n                loss.backward()\n                value_optimizer.step()  \n\n        obs = next_obs\n        if done:\n            obs = env.reset()\n    value.eval()\n    policy = lambda obs: value.act(obs,epilson=0)\n    return policy\n\nenv = gym.make(\"Blackjack-v0\")\ndef run(env,policy,k=50000):\n    wins = 0\n    for _ in range(k):\n        done=False\n        obs = env.reset()\n        while not done:\n            action = policy(obs)\n            next_obs, reward, done, _ = env.step(action)\n            obs = next_obs\n        if reward ==1:\n            wins += 1\n    print(f\"Win Percentage = {wins/k}\")\npolicy = dyna_q(env,k=500)\nrun(env, policy)","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e527e0d51876471f8bc8bcdf3217a4f6"}},"metadata":{}},{"output_type":"stream","text":"\nWin Percentage = 0.4032\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"End\")","execution_count":20,"outputs":[{"output_type":"stream","text":"End\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}