{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Workshop 1: An introduction and tabular methods.\n\n*    Notebook for Workshop 1: An introduction and tabular methods.\n*    Other parts of the workshops are available and can be found [here](https://netbrainml.github.io/workshop/).\n\n### Table of Content\n    \n   1. [Introduction](#introduction)\n   \n    *[Creating our own Environment](#native)\n\n    *[Using OpenAI Gym](#openai)\n\n   1. [Dynamic Programming](#dynamicprogramming)\n       \n     *[Policy iteration](#policyiteration)\n     \n     *[Value iteration](#valueiteration)\n     \n   3. [Monte Carlo Methods](#montecarlo)\n        \n     *[First-Visit Monte Carlo Method with Exploring Starts+ Incremental Updates](#fesimc)\n     \n     *[Every-Visit Monte Carlo Method with epilson-soft Policy + Incremental Updates](#eegimc)\n     \n     *[Off-policy Monte Carlo Method with Importance Sampling](#mcopis)\n   4. [Temporal Difference Learning](#tdlearning)\n   \n    *[SARSA](#sarsa)\n     \n    *[Expected SARSA](#esarsa)\n    \n    *[Q-Learning](#ql)\n    \n    *[Double Q-Learning](#dl)\n\n   8. [n-Step Bootstrapping](#nsbs)\n   \n    *[n-Step SARSA](#nsarsa)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pip\ndef import_or_install(package):\n    try: __import__(package)\n    except ImportError: pip.main(['install', package]) \nmodules = ['emojis','numpy', 'gym', \"git+https://github.com/MattChanTK/gym-maze.git\"]\nfor m in modules:\n    import_or_install(m)\nfrom IPython.display import clear_output\nclear_output(wait=False) ","execution_count":51,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction <a name=\"introduction\">"},{"metadata":{},"cell_type":"markdown","source":"## Creating our own Environment <a name=\"native\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport emojis\nfrom collections import defaultdict\n\nboxes =    {0:\":zero:\",\n            1:\":two:\",\n            2:\":four:\",\n            3:\":white_square_button:\",\n            4:\":black_large_square:\",\n            5:\":black_square_button:\",\n            6:\":white_check_mark:\",\n            7:\":smile:\",\n            8:\":x:\",\n            9:\":ghost:\"}\n\nclass LGM_Action_Space():\n    def __init__(self):\n        self.actions = {0:'up',1:'down',2:'left',3:'right'}\n        \n    def sample(self):\n        return np.random.choice(list(self.actions.keys()))\n\nclass LocalGlobelMaze():\n    def __init__(self, forced=False):\n        self.maze = \"\"\n        self.action_space = LGM_Action_Space()\n        self.state_space = np.arange(10)\n        self.observation_space = ['G',0,2,4]\n        self.globel = [1,3,6,8]\n        self.globel_action = None\n        self.local = [0,2,4]\n        self.win = [7]\n        self.lose = [5,9]\n        self.forced_globe = forced\n        self.reset()\n        self.nS = 10\n        self.nA = 4\n        \n    def step(self, action):\n        if self.forced_globe:\n            if self.state in self.globel and self.globel_action is not None:\n                action = self.globel_action\n                print(f\"Override action to {self.action_space.actions[action]}\")\n            if self.state in self.globel and self.globel_action is None:\n                print(f\"Globel Action set to {self.action_space.actions[action]}\")\n                self.globel_action = action\n                \n        P = np.array(self.P[self.state][action])\n        arg = np.random.choice(np.arange(len(P)),p=P[:,0])\n        self.state = int(P[arg][1])\n        return self.get_obs(self.state), P[arg][2], P[arg][3]\n    \n    def get_obs(self,state):\n        if state in self.globel:\n            return \"G\"\n        elif state in self.win or state in self.lose:\n            return 'T'\n        return state\n    \n    @property\n    def P(self):\n        \"\"\"\n        {state: {action: (prob, next_state, reward, done)}}\n        \"\"\"\n        P = {}\n        for state in self.state_space:\n            stateP = defaultdict(list)\n            for action in self.action_space.actions:\n                if action == 0: next_state = state - 5 if state>5 else state #'up'\n                elif action == 1: next_state = state + 5 if state<5 else state #'down'\n                elif action == 2: next_state = state - 1 if state not in [0,5] else state #'left'\n                elif action == 3: next_state = state + 1 if state not in [4,9] else state # 'right'\n                if state in self.win or state in self.lose:\n                    next_state = state\n                done = False\n                reward = 0\n                if next_state in self.win:\n                    done = True\n                    reward = 1\n                elif next_state in self.lose:\n                    done = True\n                    reward = -1\n                stateP[action].append((1.0, next_state, reward, done))\n            P[state] = stateP\n        return P\n    \n    def sample(self):\n        return np.random.choice(list(self.action_space.keys()))\n    \n    def reset(self):\n        self.state = np.random.choice(self.local)\n        self.done = False\n        return self.state\n        \n    def render(self):\n        self.maze = \"\"\n        for s in self.state_space:\n            if s == 5: self.maze += '\\n'\n            if s in self.local:\n                k=s/2\n                if s==self.state: k=3\n            else:\n                if s in self.win: k = 6\n                elif s in self.lose: k = 8\n                elif s in self.globel: k = 4\n                if s==self.state: k+=1\n            self.maze += boxes[k]\n        print(emojis.encode(self.maze));print(\"\\n\\n\")\n\ndef get_box(state):\n    if state == 'G':\n        return emojis.encode(boxes[4])\n    if state in env.win: k = 6\n    elif state in env.lose: k = 8\n    elif state in env.globel: k = 4\n    else: k=int(state/2)\n    return emojis.encode(boxes[k])\n\nenv = LocalGlobelMaze()\nenv.render()\n\nprint(f\"Probablity Transition Function = env.P = dict(state: dict(action: (prob, next_state, reward, done)))\\n\")\nfor state, state_info in env.P.items():\n    print(f\"In state = {state} and obs = {get_box(state)} = {env.get_obs(state)}\")\n    for action, action_info in state_info.items():\n        print(f\"\"\"\\tIf we take action {env.action_space.actions[action]},\"\"\")\n        for info in action_info:\n            prob, next_state, reward, done =info\n            print(f\"\"\"\\t\\tWe have a probablity = {prob} of the next state being {next_state} = {get_box(next_state)}, receiving reward = {reward}, and done = {done}\"\"\")","execution_count":52,"outputs":[{"output_type":"stream","text":"0ï¸âƒ£â¬›2ï¸âƒ£â¬›ðŸ”³\nâŒâ¬›âœ…â¬›âŒ\n\n\n\nProbablity Transition Function = env.P = dict(state: dict(action: (prob, next_state, reward, done)))\n\nIn state = 0 and obs = 0ï¸âƒ£ = 0\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 0 = 0ï¸âƒ£, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 5 = âŒ, receiving reward = -1, and done = True\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 0 = 0ï¸âƒ£, receiving reward = 0, and done = False\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 1 = â¬›, receiving reward = 0, and done = False\nIn state = 1 and obs = â¬› = G\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 1 = â¬›, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 6 = â¬›, receiving reward = 0, and done = False\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 0 = 0ï¸âƒ£, receiving reward = 0, and done = False\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 2 = 2ï¸âƒ£, receiving reward = 0, and done = False\nIn state = 2 and obs = 2ï¸âƒ£ = 2\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 2 = 2ï¸âƒ£, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 1 = â¬›, receiving reward = 0, and done = False\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 3 = â¬›, receiving reward = 0, and done = False\nIn state = 3 and obs = â¬› = G\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 3 = â¬›, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 8 = â¬›, receiving reward = 0, and done = False\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 2 = 2ï¸âƒ£, receiving reward = 0, and done = False\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 4 = 4ï¸âƒ£, receiving reward = 0, and done = False\nIn state = 4 and obs = 4ï¸âƒ£ = 4\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 4 = 4ï¸âƒ£, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 9 = âŒ, receiving reward = -1, and done = True\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 3 = â¬›, receiving reward = 0, and done = False\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 4 = 4ï¸âƒ£, receiving reward = 0, and done = False\nIn state = 5 and obs = âŒ = T\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 5 = âŒ, receiving reward = -1, and done = True\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 5 = âŒ, receiving reward = -1, and done = True\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 5 = âŒ, receiving reward = -1, and done = True\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 5 = âŒ, receiving reward = -1, and done = True\nIn state = 6 and obs = â¬› = G\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 1 = â¬›, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 6 = â¬›, receiving reward = 0, and done = False\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 5 = âŒ, receiving reward = -1, and done = True\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\nIn state = 7 and obs = âœ… = T\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\nIn state = 8 and obs = â¬› = G\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 3 = â¬›, receiving reward = 0, and done = False\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 8 = â¬›, receiving reward = 0, and done = False\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 7 = âœ…, receiving reward = 1, and done = True\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 9 = âŒ, receiving reward = -1, and done = True\nIn state = 9 and obs = âŒ = T\n\tIf we take action up,\n\t\tWe have a probablity = 1.0 of the next state being 9 = âŒ, receiving reward = -1, and done = True\n\tIf we take action down,\n\t\tWe have a probablity = 1.0 of the next state being 9 = âŒ, receiving reward = -1, and done = True\n\tIf we take action left,\n\t\tWe have a probablity = 1.0 of the next state being 9 = âŒ, receiving reward = -1, and done = True\n\tIf we take action right,\n\t\tWe have a probablity = 1.0 of the next state being 9 = âŒ, receiving reward = -1, and done = True\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Building a deterministic policy stochastically\")\nd_policy = {}\nfor k in env.observation_space:\n    action = env.action_space.sample()\n    print(f\"Given observation {k} = {get_box(k)}, we will always choose {env.action_space.actions[action]}\")\n    d_policy[k] = action","execution_count":53,"outputs":[{"output_type":"stream","text":"Building a deterministic policy stochastically\nGiven observation G = â¬›, we will always choose down\nGiven observation 0 = 0ï¸âƒ£, we will always choose left\nGiven observation 2 = 2ï¸âƒ£, we will always choose down\nGiven observation 4 = 4ï¸âƒ£, we will always choose down\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"obs = env.reset()\nprint(\"\\nStarting at:\", obs)\ndone = False\nt = 0\nenv.render()\nwhile not done:\n    t += 1\n    action = d_policy[obs]\n    print(\"prev:\", obs)\n    next_obs, reward, done = env.step(action)\n    print(f\"action:{env.action_space.actions[action]}, obs:{next_obs}, reward: {reward}\");env.render()\n    obs = next_obs\n    if t == 5:\n        break","execution_count":54,"outputs":[{"output_type":"stream","text":"\nStarting at: 2\n0ï¸âƒ£â¬›ðŸ”³â¬›4ï¸âƒ£\nâŒâ¬›âœ…â¬›âŒ\n\n\n\nprev: 2\naction:down, obs:T, reward: 1.0\n0ï¸âƒ£â¬›2ï¸âƒ£â¬›4ï¸âƒ£\nâŒâ¬›ðŸ˜„â¬›âŒ\n\n\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef find_value(P,gamma= 0.9,k=2):\n    value = np.zeros(env.nS)\n    for _ in range(k):\n        tmp = np.copy(value)\n        for state, state_info in P.items():\n            for action, action_info in state_info.items():\n                for info in action_info:\n                    prob, next_state, reward, done = info\n                    value[state] += 0.25*(reward + gamma*prob*tmp[next_state])\n    return value\n\ndef autolabel(rects, text, width, H):\n    for i,rect in enumerate(rects):\n        ax.text(width*i+width-1, H-0.5, round(text[i],2),\n                ha='center', va='bottom',rotation=0, weight=\"heavy\",wrap=True, fontsize=14)\nvalue = find_value(env.P)\nenv.reset()\nenv.render()\nfig = plt.figure(figsize = (10,5))\nax = fig.add_subplot(111)\n\nm = max(value)\nfor h in range(2):\n    rects=[]\n    for i in range(5):\n        width = 1\n        rects.append(ax.bar(width*i, [2-h], width, color=plt.get_cmap('RdYlGn')(value[i+5*h])))\n    autolabel(rects,value[5*h:5*h+5], width, 2-h)","execution_count":55,"outputs":[{"output_type":"stream","text":"0ï¸âƒ£â¬›2ï¸âƒ£â¬›ðŸ”³\nâŒâ¬›âœ…â¬›âŒ\n\n\n\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Using OpenAI Gym <a name=\"openai\">\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport gym\n\nenv = gym.make(\"FrozenLake-v0\")\n\n\"\"\"\nEnvironment Description:\n    The ice is slippery, so you won't always move in the direction you intend.\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n    \n    \n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n    \n    Actions are Left=0, Down=1, Right=2, Up=3\n\"\"\"\n\nprint(f\"The action space is {env.action_space}, and state space is {env.observation_space}\\n\")\n\n#Reset our environment, returns the initial state\nstate = env.reset()\nprint(f\"Our initial state is: {state}\\n\")\nenv.render() #shows the map\n\n#Randomly sample an action\naction = env.action_space.sample()\nprint(f\"Taking an action: {action}\")\n\n#Take the action, and observe the changes\nnext_state, reward, done, _ = env.step(action)\nprint(f\"The state we arrive at is {next_state}\\n\")\nprint(f\"The reward we recieved is {reward}\\n\")\nprint(f\"Are we done? {done}\\n\")\nenv.render()\n\n#Transition probablity function\nprint(f\"Probablity Transition Function = env.P = dict(state: dict(action: (prob, next_state, reward, done)))\\n\")\nfor state, state_info in env.P.items():\n    print(f\"In state = {state}\")\n    for action, action_info in state_info.items():\n        print(f\"\"\"\\tIf we take action {action},\"\"\")\n        for info in action_info:\n            prob, next_state, reward, done =info\n            print(f\"\"\"\\t\\tWe have a probablity = {prob} of the next state being {next_state}, receiving reward = {reward}, and done = {done}\"\"\")","execution_count":56,"outputs":[{"output_type":"stream","text":"The action space is Discrete(4), and state space is Discrete(16)\n\nOur initial state is: 0\n\n\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\nTaking an action: 1\nThe state we arrive at is 4\n\nThe reward we recieved is 0.0\n\nAre we done? False\n\n  (Down)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\nProbablity Transition Function = env.P = dict(state: dict(action: (prob, next_state, reward, done)))\n\nIn state = 0\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\nIn state = 1\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\nIn state = 2\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 6, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 6, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 6, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 1, receiving reward = 0.0, and done = False\nIn state = 3\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 7, receiving reward = 0.0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 7, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 7, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 3, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\nIn state = 4\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 0, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\nIn state = 5\n\tIf we take action 0,\n\t\tWe have a probablity = 1.0 of the next state being 5, receiving reward = 0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 1.0 of the next state being 5, receiving reward = 0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 1.0 of the next state being 5, receiving reward = 0, and done = True\n\tIf we take action 3,\n\t\tWe have a probablity = 1.0 of the next state being 5, receiving reward = 0, and done = True\nIn state = 6\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 7, receiving reward = 0.0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 7, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 7, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 2, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\nIn state = 7\n\tIf we take action 0,\n\t\tWe have a probablity = 1.0 of the next state being 7, receiving reward = 0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 1.0 of the next state being 7, receiving reward = 0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 1.0 of the next state being 7, receiving reward = 0, and done = True\n\tIf we take action 3,\n\t\tWe have a probablity = 1.0 of the next state being 7, receiving reward = 0, and done = True\nIn state = 8\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 12, receiving reward = 0.0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 12, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 12, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 4, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\nIn state = 9\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 5, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 8, receiving reward = 0.0, and done = False\nIn state = 10\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 6, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 11, receiving reward = 0.0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 11, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 6, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 11, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 6, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\nIn state = 11\n\tIf we take action 0,\n\t\tWe have a probablity = 1.0 of the next state being 11, receiving reward = 0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 1.0 of the next state being 11, receiving reward = 0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 1.0 of the next state being 11, receiving reward = 0, and done = True\n\tIf we take action 3,\n\t\tWe have a probablity = 1.0 of the next state being 11, receiving reward = 0, and done = True\nIn state = 12\n\tIf we take action 0,\n\t\tWe have a probablity = 1.0 of the next state being 12, receiving reward = 0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 1.0 of the next state being 12, receiving reward = 0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 1.0 of the next state being 12, receiving reward = 0, and done = True\n\tIf we take action 3,\n\t\tWe have a probablity = 1.0 of the next state being 12, receiving reward = 0, and done = True\nIn state = 13\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 12, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 12, receiving reward = 0.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 9, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 12, receiving reward = 0.0, and done = True\nIn state = 14\n\tIf we take action 0,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\tIf we take action 1,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 15, receiving reward = 1.0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 14, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 15, receiving reward = 1.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\tIf we take action 3,\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 15, receiving reward = 1.0, and done = True\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 10, receiving reward = 0.0, and done = False\n\t\tWe have a probablity = 0.3333333333333333 of the next state being 13, receiving reward = 0.0, and done = False\nIn state = 15\n\tIf we take action 0,\n\t\tWe have a probablity = 1.0 of the next state being 15, receiving reward = 0, and done = True\n\tIf we take action 1,\n\t\tWe have a probablity = 1.0 of the next state being 15, receiving reward = 0, and done = True\n\tIf we take action 2,\n\t\tWe have a probablity = 1.0 of the next state being 15, receiving reward = 0, and done = True\n\tIf we take action 3,\n\t\tWe have a probablity = 1.0 of the next state being 15, receiving reward = 0, and done = True\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(env, policy, n=1):\n    wins = 0\n    crewards = 0\n    for _ in range(n):\n        done = False\n        state = env.reset()\n        while not done:\n            action = policy[state]\n            next_state, reward, done, info = env.step(action)                        \n            crewards += reward\n            state = next_state            \n        if reward == 1.0:\n            wins += 1                \n    print(f\"\"\"\n            {'#'*20}\\n\n            Number of wins: {wins} out of {n} episodes\\n\n            Cumulative reward: {crewards} over {n} episodes\\n\n            Avg. reward per episode: {crewards/n}\\n\n            {'#'*20}\n            \"\"\")\n    \nrandom_policy = np.array([env.action_space.sample() for _ in range(env.nS)])\nprint(\"Our policy:\",random_policy, random_policy.shape)\nrun(env,random_policy, 100)","execution_count":57,"outputs":[{"output_type":"stream","text":"Our policy: [3 3 0 3 1 3 3 3 0 1 2 2 0 2 0 2] (16,)\n\n            ####################\n\n            Number of wins: 0 out of 100 episodes\n\n            Cumulative reward: 0.0 over 100 episodes\n\n            Avg. reward per episode: 0.0\n\n            ####################\n            \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_value(P,gamma= 0.9,k=10):\n    value = np.zeros(env.nS)\n    for _ in range(k):\n        tmp = np.copy(value)\n        for state, state_info in P.items():\n            for action, action_info in state_info.items():\n                for info in action_info:\n                    prob, next_state, reward, done = info\n                    if state == 15:\n                        reward = 1\n                    value[state] += 0.25*(reward + gamma*prob*tmp[next_state])\n    return value\nvalue = find_value(env.P)\n\nenv.reset()\nenv.render()\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111)\n\nfor h in range(4):\n    rects=[]\n    for i in range(4):\n        width = 1\n        rects.append(ax.bar(width*i, [4-h], width, color=plt.get_cmap('RdYlGn')(value[i+4*h])))\n    autolabel(rects,value[4*h:4*h+4], width, 4-h)","execution_count":58,"outputs":[{"output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Dynamic Programming <a name=\"dynamicprogramming\">"},{"metadata":{},"cell_type":"markdown","source":"## Policy Iteration <a name=\"policyiteration\">\n    \n    Policy Evaluation: Synchronous updates using epilson-convergence (1) and k-iterations (2)\n    Policy Improvement: Implementation follows traditional approach\n    Policy Iteration: Implementation follows traditional approach (Stopping condition = Policy stability)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using epilson-convergence\ndef policy_evaluation_1(policy, env, P, gamma=0.9, epilson=1e-3):\n    value = np.zeros(env.nS)\n    new_value = np.zeros(env.nS)\n    while True:\n        value = np.copy(new_value)\n        for state in range(env.nS): #Performing a sweep across state space\n            action = policy[state]\n            transition = np.array(P[state][action])\n            probs = transition[:,0]\n            next_states = transition[:,1].astype(int)\n            rewards = transition[:,2]\n            new_value[state] = (rewards + gamma * probs * value[next_states]).sum()\n        if np.abs(new_value-value).sum()<=epilson:\n            break\n    return new_value\n\n#k iterations\ndef policy_evaluation_2(policy, env, P, gamma=0.9, k=100):\n    value = np.zeros(env.nS)\n    new_value = np.zeros(env.nS)\n    for _ in range(k):\n        value = np.copy(new_value)\n        for state in range(env.nS): #Performing a sweep across state space\n            action = policy[state]\n            transition = np.array(P[state][action])\n            probs = transition[:,0]\n            next_states = transition[:,1].astype(int)\n            rewards = transition[:,2]\n            new_value[state] = (rewards + gamma * probs * value[next_states]).sum()\n    return new_value","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def policy_improvement(value, env, P, gamma=0.9):\n    policy = np.zeros(env.nS)\n    for state in range(env.nS):\n        q = np.zeros(env.nA)\n        for action in range(env.nA):\n            transition = np.array(P[state][action])\n            prob = transition[:,0]\n            next_state = transition[:,1].astype(int)\n            rewards = transition[:,2]\n            q[action] = (prob * (rewards + gamma * value[next_state])).sum()\n        policy[state] = q.argmax().astype(int)\n    return policy","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef policy_iteration(env, gamma=0.9):\n    #Initialization of policy and value\n    policy = np.array([env.action_space.sample() for _ in range(env.nS)])\n    P = env.P\n    while True:\n        value = policy_evaluation_1(policy, env, P, gamma)\n        updated_policy = policy_improvement(value, env, P, gamma)\n        if np.all(policy==updated_policy):\n            break\n        policy = np.copy(updated_policy)\n    return updated_policy, value","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"FrozenLake-v0\")\npolicy, value = policy_iteration(env)\n%time run(env,policy,100)","execution_count":62,"outputs":[{"output_type":"stream","text":"\n            ####################\n\n            Number of wins: 76 out of 100 episodes\n\n            Cumulative reward: 76.0 over 100 episodes\n\n            Avg. reward per episode: 0.76\n\n            ####################\n            \nCPU times: user 54.9 ms, sys: 0 ns, total: 54.9 ms\nWall time: 54.9 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.render()\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111)\n\nH,W = 4,4\n\nfor h in range(H):\n    rects=[]\n    for i in range(W):\n        width = 1\n        rects.append(ax.bar(width*i, [H-h], width, color=plt.get_cmap('RdYlGn')(value[i+4*h])))\n    autolabel(rects,value[H*h:H*h+W], width, H-h)","execution_count":63,"outputs":[{"output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Value Iteration <a name=\"valueiteration\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef find_value(env, P, gamma=0.9, epilson=1e-12):\n    value = np.zeros(env.nS)\n    new_value = np.zeros(env.nS)\n    while True:\n        value = np.copy(new_value)\n        for state in range(env.nS): #Performing a sweep across state-action space\n            q = np.zeros(env.nA)\n            for action in range(env.nA):\n                transition = np.array(P[state][action])\n                prob = transition[:,0]\n                next_state = transition[:,1].astype(int)\n                rewards = transition[:,2]\n                q[action] =  (prob * (rewards + gamma * value[next_state])).sum()\n            new_value[state] = q.max()\n        if np.abs(new_value-value).sum()<=epilson:\n            break\n    return new_value\n\ndef get_policy(value, env, P, gamma=0.9):\n    policy = np.zeros(env.nS)\n    for state in range(env.nS):\n        q = np.zeros(env.nA)\n        for action in range(env.nA):\n            transition = np.array(P[state][action])\n            prob = transition[:,0]\n            next_state = transition[:,1].astype(int)\n            rewards = transition[:,2]\n            q[action] =  (prob * (rewards + gamma * value[next_state])).sum()\n        policy[state] = q.argmax()\n    return policy\n\ndef value_iteration(env, gamma=0.9):\n    P = env.P\n    value = find_value(env, P, gamma)\n    policy = get_policy(value, env, P, gamma)\n    return policy, value","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"FrozenLake-v0\")\n\npolicy, value = value_iteration(env, gamma=1)\n\n%time run(env,policy,100)","execution_count":65,"outputs":[{"output_type":"stream","text":"\n            ####################\n\n            Number of wins: 67 out of 100 episodes\n\n            Cumulative reward: 67.0 over 100 episodes\n\n            Avg. reward per episode: 0.67\n\n            ####################\n            \nCPU times: user 89.4 ms, sys: 0 ns, total: 89.4 ms\nWall time: 89.5 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.render()\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111)\n\nH,W = 4,4\n\nfor h in range(H):\n    rects=[]\n    for i in range(W):\n        width = 1\n        rects.append(ax.bar(width*i, [H-h], width, color=plt.get_cmap('RdYlGn')(value[i+4*h])))\n    autolabel(rects,value[H*h:H*h+W], width, H-h)","execution_count":66,"outputs":[{"output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Monte Carlo Methods <a name=\"montecarlo\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport gym\nimport numpy as np\n\nenv = gym.make(\"Blackjack-v0\")\nenv.reset()\n\nprint(f\"\"\"Environment: BlackJack\\n\nCards: Face cards (Jack, Queen, King) have point value 10.\\n\\tAces can either count as 11 or 1, and it's called 'usable' at 11.\\n\nExample of observation: {env._get_obs()}\\n\nNumber of actions: {env.action_space.n}\\n\nPlayer's cards: {env.player}\\n\nDealer's cards: {env.dealer}\"\"\")\n\n\ndef run(env, policy):\n    win = 0\n    for episode in range(10000):\n        done = False\n        state = env.reset()\n        while not done:\n            action = policy[state]\n            next_state, reward, done, info = env.step(action)                        \n            if reward>=1:\n              win +=1\n            state = next_state                          \n    print(f\"Winning Percentage: {win/10000 * 100} %\")\n\npolicy = defaultdict(env.action_space.sample)\nprint(\"\\n\\nRUNNING A RANDOM POLICY\")\nrun(env,policy)","execution_count":67,"outputs":[{"output_type":"stream","text":"Environment: BlackJack\n\nCards: Face cards (Jack, Queen, King) have point value 10.\n\tAces can either count as 11 or 1, and it's called 'usable' at 11.\n\nExample of observation: (12, 6, False)\n\nNumber of actions: 2\n\nPlayer's cards: [8, 4]\n\nDealer's cards: [6, 7]\n\n\nRUNNING A RANDOM POLICY\nWinning Percentage: 28.999999999999996 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## First-Visit Monte Carlo Method with Exploring Starts + Incremental Updates <a name=\"fesimc\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_possible_state():\n    states = []\n    for player_card1 in range(1,10+1):\n        for player_card2 in range(1,10+1):\n            for dealer_card1 in range(1,10+1):\n                for dealer_card2 in range(1,10+1):\n                    states.append([player_card1,player_card2,\n                                   dealer_card1,dealer_card2])\n    return states\n\nenv.player = [4,1,2]\nenv.dealer = [2,3,4]\nprint(env._get_obs())\n\nstates = get_all_possible_state()\nprint(\"Number of possible states:\",len(states))\n\narg = np.random.choice(len(states))\nenv.player = states[arg][:2]\nenv.dealer = states[arg][2:]\nprint(f\"Players:{states[arg][:2]}, Dealer:{states[arg][2:]}\")\nprint(env._get_obs())","execution_count":68,"outputs":[{"output_type":"stream","text":"(17, 2, True)\nNumber of possible states: 10000\nPlayers:[1, 5], Dealer:[2, 9]\n(16, 2, True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\ndef fesi_mc(env, gamma = 0.98, k=10, ne=1000):\n    qvalue = defaultdict(float) \n    policy = defaultdict(env.action_space.sample)\n    pb = tqdm(range(k))\n    for i in pb:\n        all_possible_states = get_all_possible_state()\n        N = defaultdict(int)\n        \n        for n in range(ne):\n            if not n%1000: pb.set_description(f\"{i}: {n}/{ne}\")\n            #Exploring Start + Create Episode\n            arg = np.random.choice(len(all_possible_states)) #Select a starting state with equal prob.\n            start_state = all_possible_states[arg]\n            all_possible_states.pop(arg) #Sampling without replacement\n            episode = create_episode(env,start_state,policy) #Create a episode   \n            \n            #Policy Evaluation over One Episode\n            visited = []\n            G = 0\n            for t, (obs, action, reward) in enumerate(episode):\n                if (obs,action) not in visited:\n                    N[(obs,action)] += 1\n                    visited.append((obs,action))\n                    G = gamma*G + reward\n                    #Incremental QValue Improvement\n                    qvalue[(obs,action)] += 1/N[(obs,action)] * (G - qvalue[(obs,action)])\n            \n        #Greedy Policy Improvement\n        seen_sa = list(qvalue.keys())\n        for (obs,_) in seen_sa:\n            policy[obs] = np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)])\n    return policy\n\ndef create_episode(env, start_state, policy):\n    env.player = start_state[:2]\n    env.dealer = start_state[2:]\n    obs = env._get_obs()\n    action = policy[tuple(obs)]\n    next_obs, reward, done, _ = env.step(action)\n    episode = [(next_obs,action,reward)]\n    while not done:\n        obs = next_obs\n        action = policy[tuple(obs)]\n        next_obs, reward, done, _ = env.step(action)\n        episode.append((next_obs,action,reward))\n    return episode","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"Blackjack-v0\")\npolicy = fesi_mc(env,k=8, ne=9000)\n\nrun(env,policy)","execution_count":70,"outputs":[{"output_type":"stream","text":"7: 8000/9000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.29it/s]\n","name":"stderr"},{"output_type":"stream","text":"Winning Percentage: 37.24 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Every-Visit Monte Carlo Method with epilson-soft Policy + Incremental Updates <a name=\"eegimc\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def act(env, policy, obs, epilson = 0.1):\n    if epilson>np.random.random():\n        return env.action_space.sample()\n    return policy[obs]","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eegi_mc(env, gamma = 0.98, k=10, ne=1000, epilson=0.1):\n    qvalue = defaultdict(float) \n    policy = defaultdict(env.action_space.sample)\n    pb = tqdm(range(k))\n    for i in pb:\n        N = defaultdict(int)\n        for n in range(ne):\n            episode = create_episode(env,policy,epilson) #Create a episode   \n            #Policy Evaluation over One Episode\n            G = 0\n            for t, (obs, action, reward) in enumerate(episode):\n                N[(obs,action)] += 1\n                G = gamma*G + reward\n                #Incremental QValue Improvement\n                qvalue[(obs,action)] += 1/N[(obs,action)] * (G - qvalue[(obs,action)])\n        #Greedy Policy Improvement\n        seen_sa = list(qvalue.keys())\n        for (obs,_) in seen_sa:\n            policy[obs] = np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)])\n    return policy\n\ndef create_episode(env, policy, epilson = 0.1):\n    obs = env.reset()\n    action = act(env, policy, obs, epilson)\n    next_obs, reward, done, _ = env.step(action)\n    episode = [(next_obs,action,reward)]\n    while not done:\n        obs = next_obs\n        action = act(env, policy, obs, epilson)\n        next_obs, reward, done, _ = env.step(action)\n        episode.append((next_obs,action,reward))\n    return episode","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"Blackjack-v0\")\npolicy = eegi_mc(env,k=20, ne=3000)\nrun(env,policy)","execution_count":73,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.64it/s]\n","name":"stderr"},{"output_type":"stream","text":"Winning Percentage: 39.22 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Off-policy Monte Carlo Method with Importance Sampling <a name=\"mcopis\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial \nfrom scipy.special import softmax\n\nclass Policy():\n    def __init__(self, action_probs):\n        self.P = action_probs\n    def act(self, state):\n        return np.random.choice(list(self.P[state].keys()),\n                                p=softmax(list(self.P[state].values())))\n    def update_prob(self, action_probs):\n        self.P = action_probs\n    def __getitem__(self, state):\n        return np.argmax(self.P[state].values())\n\ndef init_action_prob(env, prob):\n    return {action: prob for action in range(env.action_space.n)}\n\naction_probs = defaultdict(partial(init_action_prob, env=env, prob = 0.5))\nbp = Policy(action_probs)\nfor _ in range(10):\n    obs = env.reset()\n    print(obs, bp.P[obs])","execution_count":74,"outputs":[{"output_type":"stream","text":"(12, 7, False) {0: 0.5, 1: 0.5}\n(17, 4, False) {0: 0.5, 1: 0.5}\n(9, 10, False) {0: 0.5, 1: 0.5}\n(17, 10, False) {0: 0.5, 1: 0.5}\n(16, 1, False) {0: 0.5, 1: 0.5}\n(14, 10, False) {0: 0.5, 1: 0.5}\n(21, 10, True) {0: 0.5, 1: 0.5}\n(11, 8, False) {0: 0.5, 1: 0.5}\n(12, 10, False) {0: 0.5, 1: 0.5}\n(14, 10, False) {0: 0.5, 1: 0.5}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_mc(env, gamma = 0.98, k=10, ne=1000, epilson=1e-12):\n    qvalue = defaultdict(float) \n    \n    \"\"\"\n    Create behavioral (stochastic) policy and target (deterministic) policy\n    Assume target to be epilson-greedy for importance sampling ratio calculations \n    \"\"\"\n    \n    action_probs = defaultdict(partial(init_action_prob, env=env, prob = 0.5))\n    bp = Policy(action_probs)\n    tp = defaultdict(env.action_space.sample)\n    \n    pb = tqdm(range(k))\n    for i in pb:\n        N = defaultdict(int)\n        for n in range(ne):\n            episode = create_episode(env,bp,epilson) #Create a episode   \n            #Policy Evaluation over One Episode\n            G = 0\n            for t, (obs, action, reward) in enumerate(episode):\n                N[(obs,action)] += 1\n                G = gamma*G + reward\n                #Incremental QValue Improvement\n                is_ratio = (1.0-epilson)/bp.P[obs][action] if tp[obs]==action else epilson/bp.P[obs][action]\n                qvalue[(obs,action)] += 1/N[(obs,action)] * is_ratio * (G - qvalue[(obs,action)])\n        #Greedy Policy Improvement\n        seen_sa = list(qvalue.keys())\n        for (obs,_) in seen_sa:\n            tp[obs] = np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)])\n    return policy\n            \ndef create_episode(env, policy, epilson = 0.1):\n    obs = env.reset()\n    action = policy.act(obs)\n    next_obs, reward, done, _ = env.step(action)\n    episode = [(next_obs,action,reward)]\n    while not done:\n        obs = next_obs\n        action = policy.act(obs)\n        next_obs, reward, done, _ = env.step(action)\n        episode.append((next_obs,action,reward))\n    return episode","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"Blackjack-v0\")\npolicy = is_mc(env, k=20, ne=1000)\nrun(env,policy)","execution_count":76,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.59it/s]\n","name":"stderr"},{"output_type":"stream","text":"Winning Percentage: 38.47 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Temporal Difference Learning <a name = \"tdlearning\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gym_maze.envs.maze_env import MazeEnvRandom3x3\nfrom collections import defaultdict\n\nclass Maze3x3Env(MazeEnvRandom3x3):\n    def __init__(self):\n        super(Maze3x3Env,self).__init__(enable_render=False)\n        self.enable_render = False\n    def step(self,action):\n        next_obs, reward, done, _ = super().step(action)\n        return tuple(next_obs.astype(int)), reward, done, _\n    def reset(self):\n        return tuple(super().reset().astype(int))\n        \n\nenv = Maze3x3Env()\nprint(f\"\"\"Environment: Maze 3x3\\n\nExample of observation: {env.reset()}\\n\nNumber of actions: {env.action_space.n}\\n\"\"\")\n\ndef run(env,policy, k=1000):\n    crewards = 0\n    done = False\n    state = env.reset()\n    for i in range(k):\n        action = policy[state]\n        next_state, reward, done, info = env.step(action)                        \n        crewards += reward\n        state = next_state      \n        if done:\n            break\n    print(f\"Ran for {i} steps with cumulative reward:\", crewards)\n    \npolicy = defaultdict(env.action_space.sample)\nrun(env,policy)","execution_count":77,"outputs":[{"output_type":"stream","text":"Environment: Maze 3x3\n\nExample of observation: (0, 0)\n\nNumber of actions: 4\n\nRan for 999 steps with cumulative reward: -11.111111111111075\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## SARSA <a name=\"sarsa\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nimport random\n\ndef egreedy(env, value, obs ,epilson=0.5):\n    if epilson>np.random.random():\n        return env.action_space.sample()\n    return int(np.argmax([value[(obs,action)] for action in range(env.action_space.n)]))\n\ndef sarsa(env, k=1, alpha=0.7, gamma = 0.99):\n    qvalue = defaultdict(float)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    for i in pb:\n        action = egreedy(env,qvalue,obs)\n        next_obs, reward, done, _ = env.step(action)\n        next_action = egreedy(env,qvalue,next_obs, epilson=0)\n        qvalue[(obs,action)] += alpha * (reward + gamma * qvalue[(next_obs,next_action)] - qvalue[obs, action])\n        obs = next_obs\n        if done:\n            obs = env.reset()\n    \n    policy = defaultdict(env.action_space.sample)\n    for obs in set(np.array(list(qvalue.keys()))[:,0]):\n        policy[obs]=int(np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)]))\n    return policy, qvalue\n\nenv = Maze3x3Env()\npolicy, qvalue = sarsa(env, k=150)\nrun(env,policy)","execution_count":78,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 8830.73it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 3 steps with cumulative reward: 0.9666666666666667\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gym_maze.envs.maze_env import MazeEnvRandom5x5\n\nclass Maze5x5Env(MazeEnvRandom5x5):\n    def __init__(self):\n        super(Maze5x5Env,self).__init__(enable_render=False)\n        self.enable_render = False\n    def step(self,action):\n        next_obs, reward, done, _ = super().step(action)\n        return tuple(next_obs.astype(int)), reward, done, _\n    def reset(self):\n        return tuple(super().reset().astype(int))\n\nenv = Maze5x5Env()\npolicy, qvalue = sarsa(env, k=850)\nrun(env,policy)","execution_count":79,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 850/850 [00:00<00:00, 7974.35it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 7 steps with cumulative reward: 0.972\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gym_maze.envs.maze_env import MazeEnvRandom30x30Plus\n\nclass Maze30x30EnvP(MazeEnvRandom30x30Plus):\n    def __init__(self):\n        super(Maze30x30EnvP,self).__init__(enable_render=False)\n        self.enable_render = False\n    def step(self,action):\n        next_obs, reward, done, _ = super().step(action)\n        return tuple(next_obs.astype(int)), reward, done, _\n    def reset(self):\n        return tuple(super().reset().astype(int))","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = Maze30x30EnvP()\npolicy, qvalue = sarsa(env, k=1000000)\nrun(env,policy)","execution_count":81,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [01:42<00:00, 9803.42it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 24 steps with cumulative reward: 0.9973333333333333\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Expected SARSA <a name=\"esarsa\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def expected_sarsa(env, k=1000, alpha=0.9, gamma = 0.98, epilson=0.5):\n    qvalue = defaultdict(float)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    for i in pb:\n        action = egreedy(env,qvalue,obs, 1/(i+1))\n        next_obs, reward, done, _ = env.step(action)\n        qvalue[(obs,action)] += alpha * (reward + gamma * expectation(env, qvalue, next_obs, epilson) - qvalue[obs, action])\n        obs = next_obs\n        if done:\n            state = env.reset()\n    \n    policy = defaultdict(env.action_space.sample)\n    for obs in set(np.array(list(qvalue.keys()))[:,0]):\n        policy[obs]=int(np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)]))\n    return policy, qvalue\n\ndef expectation(env, qvalue, obs, epilson):\n    values = [qvalue[(obs,a)] for a in range(env.action_space.n)]\n    prob = np.ones(env.action_space.n) * epilson/env.action_space.n\n    prob[np.argmax(values)]+=1-epilson\n    return np.sum(prob*values)\n\nenv = Maze3x3Env()\npolicy, qvalue = expected_sarsa(env, k=80)\nrun(env,policy)","execution_count":82,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 9082.76it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 3 steps with cumulative reward: 0.9666666666666667\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = Maze5x5Env()\npolicy, qvalue = expected_sarsa(env, k=750)\nrun(env,policy)","execution_count":83,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 6228.95it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 7 steps with cumulative reward: 0.972\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = Maze30x30EnvP()\npolicy, qvalue = expected_sarsa(env, k=500000)\nrun(env,policy)","execution_count":84,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500000/500000 [01:19<00:00, 6328.88it/s] ","name":"stderr"},{"output_type":"stream","text":"Ran for 38 steps with cumulative reward: 0.9957777777777778\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Q-Learning <a name=\"ql\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def q_learning(env, k=1, alpha=0.5, gamma = 0.98):\n    qvalue = defaultdict(float)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    for i in pb:\n        action = egreedy(env, qvalue, obs, 1/(i+1))\n        next_obs, reward, done, _ = env.step(action)\n        target = np.max([qvalue[(next_obs,a)] for a in range(env.action_space.n)])\n        qvalue[obs, action] += alpha * (reward + gamma * target - qvalue[obs, action])\n        obs = next_obs\n        if done:\n            obs = env.reset()\n    \n    policy = defaultdict(env.action_space.sample)\n    for obs in set(np.array(list(qvalue.keys()))[:,0]):\n        policy[obs]=int(np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)]))\n    return policy, qvalue\n\nenv = Maze3x3Env()\npolicy, qvalue = q_learning(env, k=200)\nrun(env,policy)","execution_count":85,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 5736.78it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 3 steps with cumulative reward: 0.9666666666666667\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Double Q-Learning <a name=\"dl\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def double_q_learning(env, k=1, alpha=0.5, gamma = 0.98):\n    qvalue1 = defaultdict(float)\n    qvalue2 = defaultdict(float)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    for i in pb:\n        action = epilsongreedy(env, qvalue1, qvalue2, obs, 1/(i+1))\n        next_obs, reward, done, _ = env.step(action)\n        if np.random.rand()>0.5:\n            target = np.max([qvalue2[(next_obs,a)] for a in range(env.action_space.n)])\n            qvalue1[obs, action] += alpha * (reward + gamma * target - qvalue1[obs, action])\n        else:\n            target = np.max([qvalue1[(next_obs,a)] for a in range(env.action_space.n)])\n            qvalue2[obs, action] += alpha * (reward + gamma * target - qvalue2[obs, action])\n        obs = next_obs\n        if done:\n            obs = env.reset()\n            \n    policy = defaultdict(env.action_space.sample)\n    all_obs = set(np.concatenate([list(qvalue1.keys()),list(qvalue2.keys())])[:,0])\n    for obs in all_obs:\n        policy[obs]=int(np.argmax([(qvalue1[(obs,action)]+qvalue2[(obs,action)])/2 for action in range(env.action_space.n)]))\n    return policy, qvalue1, qvalue2\n\ndef epilsongreedy(env,qvalue1,qvalue2,state,epilson=0.4):\n    if epilson>np.random.random():\n        return env.action_space.sample()\n    return int(np.argmax([(qvalue1[(state,action)]+qvalue2[(state,action)])/2 for action in range(env.action_space.n)]))\n\nenv = Maze3x3Env()\npolicy, qvalue1, qvalue2 = double_q_learning(env, k=200)\nrun(env,policy)","execution_count":86,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 6127.19it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 3 steps with cumulative reward: 0.9666666666666667\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# n-Step Bootstrapping <a name=\"nsbs\">"},{"metadata":{},"cell_type":"markdown","source":"## n-Step SARSA <a name=\"nsarsa\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def egreedy(env, value, obs ,epilson=0.5):\n    if epilson>np.random.random():\n        return env.action_space.sample()\n    return int(np.argmax([value[(obs,action)] for action in range(env.action_space.n)]))\n\ndef nstep_sarsa(env, k=1, n=10, alpha=0.5, gamma = 0.99):\n    qvalue = defaultdict(float)\n    obs = env.reset()\n    pb = tqdm(range(k))\n    buffer = []\n    for i in pb:\n        action = egreedy(env,qvalue,obs)\n        next_obs, reward, done, _ = env.step(action)\n        next_action = egreedy(env,qvalue,next_obs, epilson=1)\n        if len(buffer)==n-1:\n            G = 0\n            for r in reversed(buffer):\n                G = gamma*G + r\n            G += gamma**(n-1) * reward + gamma**n * qvalue[next_obs, next_action]\n            qvalue[(obs,action)] += alpha * (G - qvalue[obs, action])\n            buffer = []\n        else:\n            buffer.append(reward)\n        obs = next_obs\n        if done:\n            obs = env.reset()\n            buffer = []\n    \n    policy = defaultdict(env.action_space.sample)\n    for obs in set(np.array(list(qvalue.keys()))[:,0]):\n        policy[obs]=int(np.argmax([qvalue[(obs,action)] for action in range(env.action_space.n)]))\n    return policy, qvalue\n        \ndef egreedy(env, value, obs ,epilson=0.5):\n    if epilson>np.random.random():\n        return env.action_space.sample()\n    return int(np.argmax([value[(obs,action)] for action in range(env.action_space.n)]))\n    \nenv = Maze3x3Env()\npolicy, qvalue = nstep_sarsa(env, n=9, k=200*9)\nrun(env,policy)","execution_count":87,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [00:00<00:00, 7936.84it/s]","name":"stderr"},{"output_type":"stream","text":"Ran for 999 steps with cumulative reward: -11.111111111111075\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"End\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}